import json
import re
import os
import requests
import time
from typing import Dict, List, Any
from collections import defaultdict
from difflib import SequenceMatcher
from google import genai
from google.genai import types
from dotenv import load_dotenv



# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (.env íŒŒì¼ì—ì„œ API í‚¤ ë“± ì„¤ì •ê°’ ë¡œë“œ)
load_dotenv()
GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')
client = genai.Client(api_key=GOOGLE_API_KEY)

class HSDataManager:
    """
    HS ì½”ë“œ ê´€ë ¨ ë°ì´í„°ë¥¼ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤
    - HS ë¶„ë¥˜ ì‚¬ë¡€, ìœ„ì›íšŒ ê²°ì •, í˜‘ì˜íšŒ ê²°ì • ë“±ì˜ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ê´€ë¦¬
    - í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ ê¸°ëŠ¥ ì œê³µ
    - ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ìƒì„± ê¸°ëŠ¥ ì œê³µ
    """
    
    def __init__(self):
        """HSDataManager ì´ˆê¸°í™”"""
        self.data = {}  # ëª¨ë“  HS ê´€ë ¨ ë°ì´í„°ë¥¼ ì €ì¥í•˜ëŠ” ë”•ì…”ë„ˆë¦¬
        self.search_index = defaultdict(list)  # í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ì„ ìœ„í•œ ì¸ë±ìŠ¤
        self.load_all_data()  # ëª¨ë“  ë°ì´í„° íŒŒì¼ ë¡œë“œ
        self.build_search_index()  # ê²€ìƒ‰ ì¸ë±ìŠ¤ êµ¬ì¶•
    
    def load_all_data(self):
        """
        ëª¨ë“  HS ë°ì´í„° íŒŒì¼ì„ ë¡œë“œí•˜ëŠ” ë©”ì„œë“œ
        - HSë¶„ë¥˜ì‚¬ë¡€_part1~10.json íŒŒì¼ ë¡œë“œ
        - HSìœ„ì›íšŒ.json, HSí˜‘ì˜íšŒ.json íŒŒì¼ ë¡œë“œ
        - hs_classification_data_us.json íŒŒì¼ ë¡œë“œ (ë¯¸êµ­ ê´€ì„¸ì²­ í’ˆëª©ë¶„ë¥˜ ì‚¬ë¡€)
        - hs_classification_data_eu.json íŒŒì¼ ë¡œë“œ (EU ê´€ì„¸ì²­ í’ˆëª©ë¶„ë¥˜ ì‚¬ë¡€)
        """
        # HSë¶„ë¥˜ì‚¬ë¡€ íŒŒíŠ¸ ë¡œë“œ (1~10)
        for i in range(1, 11):
            try:
                with open(f'knowledge/HSë¶„ë¥˜ì‚¬ë¡€_part{i}.json', 'r', encoding='utf-8') as f:
                    self.data[f'HSë¶„ë¥˜ì‚¬ë¡€_part{i}'] = json.load(f)
            except FileNotFoundError:
                print(f'Warning: HSë¶„ë¥˜ì‚¬ë¡€_part{i}.json not found')
        
        # ê¸°íƒ€ JSON íŒŒì¼ ë¡œë“œ (ìœ„ì›íšŒ, í˜‘ì˜íšŒ ê²°ì •)
        other_files = ['knowledge/HSìœ„ì›íšŒ.json', 'knowledge/HSí˜‘ì˜íšŒ.json']
        for file in other_files:
            try:
                with open(file, 'r', encoding='utf-8') as f:
                    self.data[file.replace('.json', '')] = json.load(f)
            except FileNotFoundError:
                print(f'Warning: {file} not found')
        
        # ë¯¸êµ­ ê´€ì„¸ì²­ í’ˆëª©ë¶„ë¥˜ ì‚¬ë¡€ ë¡œë“œ
        try:
            with open('knowledge/hs_classification_data_us.json', 'r', encoding='utf-8') as f:
                self.data['hs_classification_data_us'] = json.load(f)
        except FileNotFoundError:
            print('Warning: hs_classification_data_us.json not found')
        
        # EU ê´€ì„¸ì²­ í’ˆëª©ë¶„ë¥˜ ì‚¬ë¡€ ë¡œë“œ
        try:
            with open('knowledge/hs_classification_data_eu.json', 'r', encoding='utf-8') as f:
                self.data['hs_classification_data_eu'] = json.load(f)
        except FileNotFoundError:
            print('Warning: hs_classification_data_eu.json not found')
    
    def build_search_index(self):
        """
        ê²€ìƒ‰ ì¸ë±ìŠ¤ êµ¬ì¶• ë©”ì„œë“œ
        - ê° ë°ì´í„° í•­ëª©ì—ì„œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œ
        - ì¶”ì¶œëœ í‚¤ì›Œë“œë¥¼ ì¸ë±ìŠ¤ì— ì €ì¥í•˜ì—¬ ë¹ ë¥¸ ê²€ìƒ‰ ê°€ëŠ¥
        """
        for source, items in self.data.items():
            for item in items:
                # í’ˆëª©ëª…ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
                keywords = self._extract_keywords(str(item))
                # ê° í‚¤ì›Œë“œì— ëŒ€í•´ í•´ë‹¹ ì•„ì´í…œ ì°¸ì¡° ì €ì¥
                for keyword in keywords:
                    self.search_index[keyword].append((source, item))
    
    def _extract_keywords(self, text: str) -> List[str]:
        """
        í…ìŠ¤íŠ¸ì—ì„œ ì˜ë¯¸ìˆëŠ” í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ëŠ” ë‚´ë¶€ ë©”ì„œë“œ
        Args:
            text: í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•  í…ìŠ¤íŠ¸
        Returns:
            ì¶”ì¶œëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        """
        # íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ê³µë°± ê¸°ì¤€ ë¶„ë¦¬
        words = re.sub(r'[^\w\s]', ' ', text).split()
        # ì¤‘ë³µ ì œê±° ë° ê¸¸ì´ 2 ì´ìƒì¸ ë‹¨ì–´ë§Œ ì„ íƒ
        return list(set(word for word in words if len(word) >= 2))
    
    def search(self, query: str, max_results: int = 5) -> List[Dict[str, Any]]:
        """
        ì¿¼ë¦¬ì™€ ê´€ë ¨ëœ ê°€ì¥ ì—°ê´€ì„± ë†’ì€ í•­ëª©ë“¤ì„ ê²€ìƒ‰í•˜ëŠ” ë©”ì„œë“œ
        Args:
            query: ê²€ìƒ‰í•  ì¿¼ë¦¬ ë¬¸ìì—´
            max_results: ë°˜í™˜í•  ìµœëŒ€ ê²°ê³¼ ìˆ˜ (ê¸°ë³¸ê°’: 5)
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ (ì¶œì²˜ì™€ í•­ëª© ì •ë³´ í¬í•¨)
        """
        query_keywords = self._extract_keywords(query)
        results = defaultdict(int)
        
        # ê° í‚¤ì›Œë“œì— ëŒ€í•´ ë§¤ì¹­ë˜ëŠ” í•­ëª© ì°¾ê¸°
        for keyword in query_keywords:
            for source, item in self.search_index.get(keyword, []):
                # ê°€ì¤‘ì¹˜ ê³„ì‚° (í‚¤ì›Œë“œ ë§¤ì¹­ íšŸìˆ˜ ê¸°ë°˜)
                results[(source, str(item))] += 1
        
        # ê°€ì¤‘ì¹˜ ê¸°ì¤€ ì •ë ¬
        sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)
        
        # ìƒìœ„ ê²°ê³¼ë§Œ ë°˜í™˜
        return [
            {'source': source, 'item': eval(item_str)}
            for (source, item_str), _ in sorted_results[:max_results]
        ]
    
    def search_domestic_group(self, query: str, group_idx: int, max_results: int = 3) -> List[Dict[str, Any]]:
        """êµ­ë‚´ HS ë¶„ë¥˜ ë°ì´í„° ê·¸ë£¹ë³„ ê²€ìƒ‰ ë©”ì„œë“œ"""
        query_keywords = self._extract_keywords(query)
        results = defaultdict(int)

        # ê·¸ë£¹ë³„ ë°ì´í„° ì†ŒìŠ¤ ì •ì˜ (5ê°œ ê·¸ë£¹)
        group_sources = [
            ['HSë¶„ë¥˜ì‚¬ë¡€_part1', 'HSë¶„ë¥˜ì‚¬ë¡€_part2'],  # ê·¸ë£¹1
            ['HSë¶„ë¥˜ì‚¬ë¡€_part3', 'HSë¶„ë¥˜ì‚¬ë¡€_part4'],  # ê·¸ë£¹2
            ['HSë¶„ë¥˜ì‚¬ë¡€_part5', 'HSë¶„ë¥˜ì‚¬ë¡€_part6'],  # ê·¸ë£¹3
            ['HSë¶„ë¥˜ì‚¬ë¡€_part7', 'HSë¶„ë¥˜ì‚¬ë¡€_part8'],  # ê·¸ë£¹4
            ['HSë¶„ë¥˜ì‚¬ë¡€_part9', 'HSë¶„ë¥˜ì‚¬ë¡€_part10', 'knowledge/HSìœ„ì›íšŒ', 'knowledge/HSí˜‘ì˜íšŒ']  # ê·¸ë£¹5
        ]
        sources = group_sources[group_idx]

        for keyword in query_keywords:
            for source, item in self.search_index.get(keyword, []):
                if source in sources:
                    results[(source, str(item))] += 1

        sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)
        return [
            {'source': source, 'item': eval(item_str)}
            for (source, item_str), _ in sorted_results[:max_results]
        ]

    def get_domestic_context_group(self, query: str, group_idx: int) -> str:
        """êµ­ë‚´ HS ë¶„ë¥˜ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸(ê·¸ë£¹ë³„)ë¥¼ ìƒì„±í•˜ëŠ” ë©”ì„œë“œ"""
        results = self.search_domestic_group(query, group_idx)
        context = []
        for result in results:
            context.append(f"ì¶œì²˜: {result['source']} (êµ­ë‚´ ê´€ì„¸ì²­)\ní•­ëª©: {json.dumps(result['item'], ensure_ascii=False)}")
        return "\n\n".join(context)

    def search_overseas_group(self, query: str, group_idx: int, max_results: int = 3) -> List[Dict[str, Any]]:
        """í•´ì™¸ HS ë¶„ë¥˜ ë°ì´í„° ê·¸ë£¹ë³„ ê²€ìƒ‰ ë©”ì„œë“œ"""
        query_keywords = self._extract_keywords(query)
        results = defaultdict(int)
        
        # í•´ì™¸ ë°ì´í„°ë¥¼ ê·¸ë£¹ë³„ë¡œ ë¶„í•  ì²˜ë¦¬
        if group_idx < 3:  # ê·¸ë£¹ 0,1,2ëŠ” ë¯¸êµ­ ë°ì´í„°
            target_source = 'hs_classification_data_us'
            # ë¯¸êµ­ ë°ì´í„°ë¥¼ 3ë“±ë¶„
            us_data = self.data.get(target_source, [])
            chunk_size = len(us_data) // 3
            start_idx = group_idx * chunk_size
            end_idx = start_idx + chunk_size if group_idx < 2 else len(us_data)
            target_items = us_data[start_idx:end_idx]
        else:  # ê·¸ë£¹ 3,4ëŠ” EU ë°ì´í„°
            target_source = 'hs_classification_data_eu'
            # EU ë°ì´í„°ë¥¼ 2ë“±ë¶„
            eu_data = self.data.get(target_source, [])
            chunk_size = len(eu_data) // 2
            eu_group_idx = group_idx - 3  # 0 or 1
            start_idx = eu_group_idx * chunk_size
            end_idx = start_idx + chunk_size if eu_group_idx < 1 else len(eu_data)
            target_items = eu_data[start_idx:end_idx]
        
        # í•´ë‹¹ ê·¸ë£¹ ë°ì´í„°ì—ì„œë§Œ ê²€ìƒ‰
        for keyword in query_keywords:
            for source, item in self.search_index.get(keyword, []):
                if source == target_source and item in target_items:
                    results[(source, str(item))] += 1
        
        sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)
        return [
            {'source': source, 'item': eval(item_str)}
            for (source, item_str), _ in sorted_results[:max_results]
        ]

    def get_overseas_context_group(self, query: str, group_idx: int) -> str:
        """í•´ì™¸ HS ë¶„ë¥˜ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸(ê·¸ë£¹ë³„)ë¥¼ ìƒì„±í•˜ëŠ” ë©”ì„œë“œ"""
        results = self.search_overseas_group(query, group_idx)
        context = []
        
        for result in results:
            # ì¶œì²˜ì— ë”°ë¼ êµ­ê°€ êµ¬ë¶„
            if result['source'] == 'hs_classification_data_us':
                country = "ë¯¸êµ­ ê´€ì„¸ì²­"
            elif result['source'] == 'hs_classification_data_eu':
                country = "EU ê´€ì„¸ì²­"
            else:
                country = "í•´ì™¸ ê´€ì„¸ì²­"
                
            context.append(f"ì¶œì²˜: {result['source']} ({country})\ní•­ëª©: {json.dumps(result['item'], ensure_ascii=False)}")
        
        return "\n\n".join(context)
    
    def search_domestic(self, query: str, max_results: int = 5) -> List[Dict[str, Any]]:
        """êµ­ë‚´ HS ë¶„ë¥˜ ë°ì´í„°ì—ì„œë§Œ ê²€ìƒ‰í•˜ëŠ” ë©”ì„œë“œ"""
        query_keywords = self._extract_keywords(query)
        results = defaultdict(int)
        
        # êµ­ë‚´ ë°ì´í„° ì†ŒìŠ¤ë§Œ í•„í„°ë§
        domestic_sources = [
            'HSë¶„ë¥˜ì‚¬ë¡€_part1', 'HSë¶„ë¥˜ì‚¬ë¡€_part2', 'HSë¶„ë¥˜ì‚¬ë¡€_part3', 'HSë¶„ë¥˜ì‚¬ë¡€_part4', 'HSë¶„ë¥˜ì‚¬ë¡€_part5',
            'HSë¶„ë¥˜ì‚¬ë¡€_part6', 'HSë¶„ë¥˜ì‚¬ë¡€_part7', 'HSë¶„ë¥˜ì‚¬ë¡€_part8', 'HSë¶„ë¥˜ì‚¬ë¡€_part9', 'HSë¶„ë¥˜ì‚¬ë¡€_part10',
            'knowledge/HSìœ„ì›íšŒ', 'knowledge/HSí˜‘ì˜íšŒ'
        ]
        
        for keyword in query_keywords:
            for source, item in self.search_index.get(keyword, []):
                # êµ­ë‚´ ë°ì´í„° ì†ŒìŠ¤ë§Œ í¬í•¨
                if source in domestic_sources:
                    results[(source, str(item))] += 1
        
        sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)
        
        return [
            {'source': source, 'item': eval(item_str)}
            for (source, item_str), _ in sorted_results[:max_results]
        ]
    
    def get_domestic_context(self, query: str) -> str:
        """êµ­ë‚´ HS ë¶„ë¥˜ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ë©”ì„œë“œ"""
        results = self.search_domestic(query)
        context = []
        
        for result in results:
            context.append(f"ì¶œì²˜: {result['source']} (êµ­ë‚´ ê´€ì„¸ì²­)\ní•­ëª©: {json.dumps(result['item'], ensure_ascii=False)}")
        
        return "\n\n".join(context)
    
    def search_overseas_improved(self, query: str, max_results: int = 5) -> List[Dict[str, Any]]:
        """í•´ì™¸ HS ë¶„ë¥˜ ë°ì´í„°ì—ì„œë§Œ ê²€ìƒ‰í•˜ëŠ” ê°œì„ ëœ ë©”ì„œë“œ (search_index í™œìš©)"""
        query_keywords = self._extract_keywords(query)
        results = defaultdict(int)
        
        # í•´ì™¸ ë°ì´í„° ì†ŒìŠ¤ë§Œ í•„í„°ë§
        overseas_sources = ['hs_classification_data_us', 'hs_classification_data_eu']
        
        for keyword in query_keywords:
            for source, item in self.search_index.get(keyword, []):
                # í•´ì™¸ ë°ì´í„° ì†ŒìŠ¤ë§Œ í¬í•¨
                if source in overseas_sources:
                    results[(source, str(item))] += 1
        
        sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)
        
        return [
            {'source': source, 'item': eval(item_str)}
            for (source, item_str), _ in sorted_results[:max_results]
        ]
    
    def get_domestic_context(self, query: str) -> str:
        """êµ­ë‚´ HS ë¶„ë¥˜ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ë©”ì„œë“œ"""
        results = self.search_domestic(query)
        context = []
        
        for result in results:
            context.append(f"ì¶œì²˜: {result['source']} (êµ­ë‚´ ê´€ì„¸ì²­)\ní•­ëª©: {json.dumps(result['item'], ensure_ascii=False)}")
        
        return "\n\n".join(context)


    def get_relevant_context(self, query: str) -> str:
        """
        ì¿¼ë¦¬ì— ê´€ë ¨ëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ë©”ì„œë“œ
        Args:
            query: ì»¨í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•  ì¿¼ë¦¬ ë¬¸ìì—´
        Returns:
            ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ (ì¶œì²˜ì™€ í•­ëª© ì •ë³´ í¬í•¨)
        """
        results = self.search(query)
        context = []
        
        for result in results:
            context.append(f"ì¶œì²˜: {result['source']}\ní•­ëª©: {json.dumps(result['item'], ensure_ascii=False)}")
        
        return "\n\n".join(context)
    
    def get_overseas_context_improved(self, query: str) -> str:
        """í•´ì™¸ HS ë¶„ë¥˜ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ê°œì„ ëœ ë©”ì„œë“œ"""
        results = self.search_overseas_improved(query)
        context = []
        
        for result in results:
            # ì¶œì²˜ì— ë”°ë¼ êµ­ê°€ êµ¬ë¶„
            if result['source'] == 'hs_classification_data_us':
                country = "ë¯¸êµ­ ê´€ì„¸ì²­"
            elif result['source'] == 'hs_classification_data_eu':
                country = "EU ê´€ì„¸ì²­"
            else:
                country = "í•´ì™¸ ê´€ì„¸ì²­"
                
            context.append(f"ì¶œì²˜: {result['source']} ({country})\ní•­ëª©: {json.dumps(result['item'], ensure_ascii=False)}")
        
        return "\n\n".join(context)

# HTML íƒœê·¸ ì œê±° ë° í…ìŠ¤íŠ¸ ì •ì œ í•¨ìˆ˜
def clean_text(text):
    # HTML íƒœê·¸ ì œê±° (ë” ì—„ê²©í•œ ì •ê·œì‹ íŒ¨í„´ ì‚¬ìš©)
    text = re.sub(r'<[^>]+>', '', text)  # ëª¨ë“  HTML íƒœê·¸ ì œê±°
    text = re.sub(r'\s*</div>\s*$', '', text)  # ëì— ìˆëŠ” </div> íƒœê·¸ ì œê±°
    return text.strip()

# HS ì½”ë“œ ì¶”ì¶œ íŒ¨í„´ ì •ì˜ ë° í•¨ìˆ˜
# ë” ìœ ì—°í•œ HS ì½”ë“œ ì¶”ì¶œ íŒ¨í„´
HS_PATTERN = re.compile(
    r'(?:HS\s*)?(\d{4}(?:[.-]?\d{2}(?:[.-]?\d{2}(?:[.-]?\d{2})?)?)?)',
    flags=re.IGNORECASE
)

def extract_hs_codes(text):
    """
    ì—¬ëŸ¬ HS ì½”ë“œë¥¼ ì¶”ì¶œí•˜ê³ , ì¤‘ë³µ ì œê±° ë° ìˆ«ìë§Œ ë‚¨ê²¨ í‘œì¤€í™”
    ê°œì„ ì‚¬í•­:
    - ë‹¨ì–´ ê²½ê³„(\b) ì œê±°ë¡œ ë” ìœ ì—°í•œ ë§¤ì¹­
    - ìˆ«ìë§Œ ìˆëŠ” ê²½ìš°ë„ ì²˜ë¦¬ ê°€ëŠ¥
    - ìµœì†Œ 4ìë¦¬ ìˆ«ì ì²´í¬ ì¶”ê°€
    """
    matches = HS_PATTERN.findall(text)
    hs_codes = []
    
    for raw in matches:
        # ìˆ«ìë§Œ ë‚¨ê¸°ê¸°
        code = re.sub(r'\D', '', raw)
        # ìµœì†Œ 4ìë¦¬ì´ê³  ì¤‘ë³µì´ ì•„ë‹Œ ê²½ìš°ë§Œ ì¶”ê°€
        if len(code) >= 4 and code not in hs_codes:
            hs_codes.append(code)
    
    # ë§Œì•½ ìœ„ íŒ¨í„´ìœ¼ë¡œ ì°¾ì§€ ëª»í•˜ê³ , ì…ë ¥ì´ 4ìë¦¬ ì´ìƒì˜ ìˆ«ìë¡œë§Œ êµ¬ì„±ëœ ê²½ìš°
    if not hs_codes:
        # ìˆœìˆ˜ ìˆ«ìë§Œ ìˆëŠ” ê²½ìš° ì²´í¬
        numbers_only = re.findall(r'\d{4,}', text)
        for num in numbers_only:
            if num not in hs_codes:
                hs_codes.append(num)
    
    return hs_codes

def extract_and_store_text(json_file):
    """JSON íŒŒì¼ì—ì„œ head1ê³¼ textë¥¼ ì¶”ì¶œí•˜ì—¬ ë³€ìˆ˜ì— ì €ì¥"""
    try:
        # JSON íŒŒì¼ ì½ê¸°
        with open(json_file, 'r', encoding='utf-8') as file:
            data = json.load(file)
        
        # ë°ì´í„°ë¥¼ ë³€ìˆ˜ì— ì €ì¥
        extracted_data = []
        for item in data:
            head1 = item.get('head1', '')
            text = item.get('text', '')
            if head1 or text:
                extracted_data.append(f"{head1}\n{text}")
        
        return extracted_data
    except Exception as e:
        print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []

# í†µì¹™ ë°ì´í„° ë¡œë“œ (ì¬ì‚¬ìš©ì„ ìœ„í•œ ì „ì—­ ë³€ìˆ˜)
general_explanation = extract_and_store_text('knowledge/í†µì¹™_grouped.json')

def lookup_hscode(hs_code, json_file):
    """HS ì½”ë“œì— ëŒ€í•œ í•´ì„¤ ì •ë³´ë¥¼ ì¡°íšŒí•˜ëŠ” í•¨ìˆ˜"""
    try:
        with open(json_file, 'r', encoding='utf-8') as file:
            data = json.load(file)
        
        # ê° ì„¤ëª… ìœ í˜•ë³„ ì´ˆê¸°ê°’ ì„¤ì •
        part_explanation = {"text": "í•´ë‹¹ ë¶€ì— ëŒ€í•œ ì„¤ëª…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
        chapter_explanation = {"text": "í•´ë‹¹ ë¥˜ì— ëŒ€í•œ ì„¤ëª…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
        sub_explanation = {"text": "í•´ë‹¹ í˜¸ì— ëŒ€í•œ ì„¤ëª…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

        # 1) ë¥˜(é¡) key: "ì œ00ë¥˜"
        chapter_key = f"ì œ{int(hs_code[:2])}ë¥˜"
        chapter_explanation = next((g for g in data if g.get('header2') == chapter_key), chapter_explanation)

        # 2) í˜¸ key: "00.00"
        sub_key = f"{hs_code[:2]}.{hs_code[2:]}"
        sub_explanation = next((g for g in data if g.get('header2') == sub_key), sub_explanation)

        # 3) ë¶€(éƒ¨) key: "ì œ00ë¶€"
        part_key = chapter_explanation.get('header1') if chapter_explanation else None
        part_explanation = next((g for g in data if (g.get('header1') == part_key)&(re.sub(r'ì œ\s*(\d+)\s*ë¶€', r'ì œ\1ë¶€', g.get('header1')) == part_key)), None)
        
        return part_explanation, chapter_explanation, sub_explanation
    
    except Exception as e:
        print(f"HS ì½”ë“œ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        return ({"text": "ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}, {"text": "ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}, {"text": "ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."})

def get_hs_explanations(hs_codes):
    """ì—¬ëŸ¬ HS ì½”ë“œì— ëŒ€í•œ í•´ì„¤ì„ ì·¨í•©í•˜ëŠ” í•¨ìˆ˜ (ë§ˆí¬ë‹¤ìš´ í˜•ì‹)"""
    all_explanations = ""
    for hs_code in hs_codes:
        explanation, type_explanation, number_explanation = lookup_hscode(hs_code, 'knowledge/grouped_11_end.json')

        if explanation and type_explanation and number_explanation:
            all_explanations += f"\n\n# HS ì½”ë“œ {hs_code} í•´ì„¤\n\n"
            all_explanations += f"## ğŸ“‹ í•´ì„¤ì„œ í†µì¹™\n\n"
            
            # í†µì¹™ ë‚´ìš©ì„ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì •ë¦¬
            if general_explanation:
                for i, rule in enumerate(general_explanation[:5], 1):  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
                    all_explanations += f"### í†µì¹™ {i}\n{rule}\n\n"
            
            all_explanations += f"## ğŸ“‚ ë¶€(éƒ¨) í•´ì„¤\n\n{explanation['text']}\n\n"
            all_explanations += f"## ğŸ“š ë¥˜(é¡) í•´ì„¤\n\n{type_explanation['text']}\n\n"
            all_explanations += f"## ğŸ“ í˜¸(è™Ÿ) í•´ì„¤\n\n{number_explanation['text']}\n\n"
            all_explanations += "---\n"  # êµ¬ë¶„ì„  ì¶”ê°€
    
    return all_explanations

class TariffTableSearcher:
    def __init__(self):
        self.tariff_data = []
        self.load_tariff_table()
    
    def load_tariff_table(self):
        """ê´€ì„¸ìœ¨í‘œ ë°ì´í„° ë¡œë“œ"""
        try:
            with open('knowledge/hstable.json', 'r', encoding='utf-8') as f:
                self.tariff_data = json.load(f)
        except FileNotFoundError:
            print("Warning: hstable.json not found")
            self.tariff_data = []
    
    def calculate_similarity(self, query, text):
        """í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        if not query or not text:
            return 0.0
        return SequenceMatcher(None, query.lower(), text.lower()).ratio()
    
    def search_by_tariff_table(self, query, top_n=10):
        """ê´€ì„¸ìœ¨í‘œì—ì„œ ìœ ì‚¬ë„ ê¸°ë°˜ HSì½”ë“œ í›„ë³´ ê²€ìƒ‰"""
        candidates = []
        
        for item in self.tariff_data:
            hs_code = item.get('í’ˆëª©ë²ˆí˜¸', '')
            korean_name = item.get('í•œê¸€í’ˆëª…', '')
            english_name = item.get('ì˜ë¬¸í’ˆëª…', '')
            
            # í•œê¸€í’ˆëª…ê³¼ ì˜ë¬¸í’ˆëª…ì—ì„œ ìœ ì‚¬ë„ ê³„ì‚°
            korean_sim = self.calculate_similarity(query, korean_name)
            english_sim = self.calculate_similarity(query, english_name)
            
            # ìµœê³  ìœ ì‚¬ë„ ì‚¬ìš©
            max_similarity = max(korean_sim, english_sim)
            
            if max_similarity > 0.1:  # ìµœì†Œ ì„ê³„ê°’
                candidates.append({
                    'hs_code': hs_code,
                    'korean_name': korean_name,
                    'english_name': english_name,
                    'similarity': max_similarity,
                    'matched_field': 'korean' if korean_sim > english_sim else 'english'
                })
        
        # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ Nê°œ ë°˜í™˜
        candidates.sort(key=lambda x: x['similarity'], reverse=True)
        return candidates[:top_n]

class ParallelHSSearcher:
    def __init__(self, hs_manager):
        self.hs_manager = hs_manager
        self.tariff_searcher = TariffTableSearcher()
    
    def parallel_search(self, query, logger):
        """ë³‘ë ¬ì  HSì½”ë“œ ê²€ìƒ‰"""
        
        # ê²½ë¡œ 1: ê´€ì„¸ìœ¨í‘œ â†’ í•´ì„¤ì„œ (2ë‹¨ê³„)
        logger.log_actual("SEARCH", "Path 1: Tariff Table â†’ Manual search starting...")
        path1_results = self.tariff_to_manual_search(query, logger)
        
        # ê²½ë¡œ 2: í•´ì„¤ì„œ ì§ì ‘ ê²€ìƒ‰ (ê¸°ì¡´ ë°©ë²•)
        logger.log_actual("SEARCH", "Path 2: Direct manual search starting...")
        path2_results = self.direct_manual_search(query, logger)
        
        # ê²°ê³¼ ì¢…í•©
        logger.log_actual("AI", "Consolidating parallel search results...")
        final_results = self.consolidate_results(path1_results, path2_results, logger)
        
        return final_results
    
    def tariff_to_manual_search(self, query, logger):
        """ê²½ë¡œ 1: ê´€ì„¸ìœ¨í‘œ â†’ í•´ì„¤ì„œ"""
        # 1ë‹¨ê³„: ê´€ì„¸ìœ¨í‘œì—ì„œ HSì½”ë“œ í›„ë³´ ì„ ì •
        tariff_start = time.time()
        hs_candidates = self.tariff_searcher.search_by_tariff_table(query, top_n=15)
        tariff_time = time.time() - tariff_start
        
        logger.log_actual("DATA", f"Tariff table search completed", 
                         f"{len(hs_candidates)} candidates in {tariff_time:.2f}s")
        
        if not hs_candidates:
            return []
            
        # ìƒìœ„ í›„ë³´ë“¤ì˜ HSì½”ë“œ ë¦¬ìŠ¤íŠ¸ ìƒì„±
        candidate_codes = [item['hs_code'] for item in hs_candidates[:10]]
        logger.log_actual("INFO", f"Top HS candidates from tariff", 
                         f"{', '.join(candidate_codes[:5])}...")
        
        # 2ë‹¨ê³„: í•´ë‹¹ HSì½”ë“œë“¤ì„ í•´ì„¤ì„œì—ì„œ ê²€ìƒ‰
        manual_start = time.time()
        manual_results = []
        
        for candidate in hs_candidates[:10]:
            hs_code = candidate['hs_code']
            # í•´ì„¤ì„œì—ì„œ í•´ë‹¹ HSì½”ë“œ ê´€ë ¨ ë‚´ìš© ê²€ìƒ‰
            manual_content = self.search_manual_by_hs_code(hs_code, query)
            if manual_content:
                manual_results.append({
                    'hs_code': hs_code,
                    'tariff_similarity': candidate['similarity'],
                    'tariff_name': candidate['korean_name'],
                    'manual_content': manual_content,
                    'source': 'tariff_to_manual'
                })
        
        manual_time = time.time() - manual_start
        logger.log_actual("SUCCESS", f"Manual search for candidates completed", 
                         f"{len(manual_results)} results in {manual_time:.2f}s")
        
        return manual_results
    
    def search_manual_by_hs_code(self, hs_code, query):
        """íŠ¹ì • HSì½”ë“œì— ëŒ€í•œ í•´ì„¤ì„œ ë‚´ìš© ê²€ìƒ‰"""
        try:
            explanation, type_explanation, number_explanation = lookup_hscode(hs_code, 'knowledge/grouped_11_end.json')
            
            content = ""
            if explanation and explanation.get('text'):
                content += f"ë¶€ í•´ì„¤: {explanation['text']}\n"
            if type_explanation and type_explanation.get('text'):
                content += f"ë¥˜ í•´ì„¤: {type_explanation['text']}\n"
            if number_explanation and number_explanation.get('text'):
                content += f"í˜¸ í•´ì„¤: {number_explanation['text']}\n"
                
            return content if content else None
        except:
            return None
    
    def direct_manual_search(self, query, logger):
        """ê²½ë¡œ 2: í•´ì„¤ì„œ ì§ì ‘ ê²€ìƒ‰ (ê¸°ì¡´ ë°©ë²•)"""
        manual_start = time.time()
        
        # ê¸°ì¡´ multi-agent ë°©ì‹ í™œìš©
        direct_results = []
        for i in range(5):  # 5ê°œ ê·¸ë£¹ ê²€ìƒ‰
            group_results = self.hs_manager.search_domestic_group(query, i, max_results=2)
            for result in group_results:
                direct_results.append({
                    'source_group': i,
                    'content': result,
                    'source': 'direct_manual'
                })
        
        manual_time = time.time() - manual_start
        logger.log_actual("SUCCESS", f"Direct manual search completed", 
                         f"{len(direct_results)} results in {manual_time:.2f}s")
        
        return direct_results
    
    def extract_hs_codes_from_content(self, content):
        """í•´ì„¤ì„œ ë‚´ìš©ì—ì„œ HSì½”ë“œ ì¶”ì¶œ"""
        if isinstance(content, dict):
            text_content = json.dumps(content, ensure_ascii=False)
        else:
            text_content = str(content)
            
        # HSì½”ë“œ íŒ¨í„´ ì¶”ì¶œ
        codes = extract_hs_codes(text_content)
        return codes[:3]  # ìµœëŒ€ 3ê°œë§Œ
    
    def consolidate_results(self, path1_results, path2_results, logger):
        """ë‘ ê²½ë¡œì˜ ê²°ê³¼ë¥¼ ì¢…í•©"""
        consolidation_start = time.time()
        
        # ê°€ì¤‘ì¹˜ ì„¤ì •
        TARIFF_WEIGHT = 0.4  # ê´€ì„¸ìœ¨í‘œ ê²½ë¡œ ê°€ì¤‘ì¹˜
        MANUAL_WEIGHT = 0.6  # í•´ì„¤ì„œ ì§ì ‘ ê²½ë¡œ ê°€ì¤‘ì¹˜
        
        final_scores = defaultdict(float)
        result_details = {}
        
        # ê²½ë¡œ 1 ê²°ê³¼ ì²˜ë¦¬ (ê´€ì„¸ìœ¨í‘œ â†’ í•´ì„¤ì„œ)
        for result in path1_results:
            hs_code = result['hs_code']
            # ê´€ì„¸ìœ¨í‘œ ìœ ì‚¬ë„ * ê°€ì¤‘ì¹˜
            score = result['tariff_similarity'] * TARIFF_WEIGHT
            final_scores[hs_code] += score
            
            if hs_code not in result_details:
                result_details[hs_code] = {
                    'hs_code': hs_code,
                    'tariff_name': result.get('tariff_name', ''),
                    'manual_content': result.get('manual_content', ''),
                    'path1_score': score,
                    'path2_score': 0,
                    'sources': ['tariff_to_manual']
                }
            else:
                result_details[hs_code]['sources'].append('tariff_to_manual')
        
        # ê²½ë¡œ 2 ê²°ê³¼ ì²˜ë¦¬ (í•´ì„¤ì„œ ì§ì ‘)
        for result in path2_results:
            # HSì½”ë“œ ì¶”ì¶œ ë¡œì§ (í•´ì„¤ì„œ ë‚´ìš©ì—ì„œ)
            extracted_codes = self.extract_hs_codes_from_content(result['content'])
            
            for hs_code in extracted_codes:
                # í•´ì„¤ì„œ ì§ì ‘ ê²€ìƒ‰ ì ìˆ˜ (ë¹ˆë„ ê¸°ë°˜)
                score = 0.5 * MANUAL_WEIGHT  # ê¸°ë³¸ ì ìˆ˜
                final_scores[hs_code] += score
                
                if hs_code not in result_details:
                    result_details[hs_code] = {
                        'hs_code': hs_code,
                        'tariff_name': '',
                        'manual_content': str(result['content']),
                        'path1_score': 0,
                        'path2_score': score,
                        'sources': ['direct_manual']
                    }
                else:
                    result_details[hs_code]['path2_score'] += score
                    if 'direct_manual' not in result_details[hs_code]['sources']:
                        result_details[hs_code]['sources'].append('direct_manual')
        
        # ìµœì¢… ìˆœìœ„ ì •ë ¬
        sorted_results = sorted(final_scores.items(), key=lambda x: x[1], reverse=True)
        
        consolidation_time = time.time() - consolidation_start
        logger.log_actual("SUCCESS", f"Results consolidation completed", 
                         f"{len(sorted_results)} unique HS codes in {consolidation_time:.2f}s")
        
        # ìƒìœ„ 5ê°œ ê²°ê³¼ ë°˜í™˜
        top_results = []
        for hs_code, final_score in sorted_results[:5]:
            if hs_code in result_details:
                details = result_details[hs_code]
                details['final_score'] = final_score
                details['confidence'] = 'HIGH' if len(details['sources']) > 1 else 'MEDIUM'
                top_results.append(details)
        
        return top_results
    
    def create_enhanced_context(self, search_results):
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        context = ""
        
        for i, result in enumerate(search_results, 1):
            context += f"\n=== í›„ë³´ {i}: HSì½”ë“œ {result['hs_code']} ===\n"
            context += f"ì‹ ë¢°ë„: {result['confidence']}\n"
            context += f"ìµœì¢…ì ìˆ˜: {result['final_score']:.3f}\n"
            
            if result['tariff_name']:
                context += f"ê´€ì„¸ìœ¨í‘œ í’ˆëª©ëª…: {result['tariff_name']}\n"
            
            context += f"ê²€ìƒ‰ê²½ë¡œ: {', '.join(result['sources'])}\n"
            
            if result['manual_content']:
                context += f"í•´ì„¤ì„œ ë‚´ìš©:\n{result['manual_content'][:500]}...\n"
            
            context += "\n"
        
        return context

def handle_hs_manual_with_parallel_search(user_input, context, hs_manager, logger):
    """ë³‘ë ¬ ê²€ìƒ‰ì„ í™œìš©í•œ HS í•´ì„¤ì„œ ë¶„ì„"""
    
    # ë³‘ë ¬ ê²€ìƒ‰ ìˆ˜í–‰
    parallel_searcher = ParallelHSSearcher(hs_manager)
    search_results = parallel_searcher.parallel_search(user_input, logger)
    
    # ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    enhanced_context = parallel_searcher.create_enhanced_context(search_results)
    
    logger.log_actual("INFO", f"Enhanced context prepared", f"{len(enhanced_context)} chars")
    
    # Geminiì— ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = f"""{context}

[ë³‘ë ¬ ê²€ìƒ‰ ê²°ê³¼]
{enhanced_context}

ì‚¬ìš©ì ì§ˆë¬¸: {user_input}

ìœ„ì˜ ë³‘ë ¬ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒì„ í¬í•¨í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”:

1. **ê°€ì¥ ì í•©í•œ HS ì½”ë“œ ì¶”ì²œ**
   - ìµœê³  ì‹ ë¢°ë„ì˜ HSì½”ë“œì™€ ê·¸ ê·¼ê±°
   - ê´€ì„¸ìœ¨í‘œ í’ˆëª©ëª…ê³¼ í•´ì„¤ì„œ ì„¤ëª… ì¢…í•©

2. **ë¶„ë¥˜ ê·¼ê±° ë° ë¶„ì„**
   - ê´€ì„¸ìœ¨í‘œ ê¸°ë°˜ ê²€ìƒ‰ ê²°ê³¼
   - í•´ì„¤ì„œ ê¸°ë°˜ ê²€ìƒ‰ ê²°ê³¼
   - ë‘ ê²€ìƒ‰ ê²½ë¡œì˜ ì¼ì¹˜ì„± ë¶„ì„

3. **ì‹ ë¢°ë„ í‰ê°€**
   - HIGH: ë‘ ê²€ìƒ‰ ê²½ë¡œ ëª¨ë‘ì—ì„œ ë°œê²¬
   - MEDIUM: í•œ ê²€ìƒ‰ ê²½ë¡œì—ì„œë§Œ ë°œê²¬
   - ê° í›„ë³´ì˜ ì‹ ë¢°ë„ì™€ ì ìˆ˜

4. **ì¶”ê°€ ê³ ë ¤ì‚¬í•­**
   - ìœ ì‚¬ í’ˆëª©ê³¼ì˜ êµ¬ë¶„ ê¸°ì¤€
   - ë¶„ë¥˜ ì‹œ ì£¼ì˜ì 
   - í•„ìš” ì‹œ ì¶”ê°€ ì •ë³´ ìš”ì²­ ì‚¬í•­

ë‹µë³€ì€ ì „ë¬¸ì ì´ë©´ì„œë„ ì´í•´í•˜ê¸° ì‰½ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.
"""
    
    # Gemini ì²˜ë¦¬
    logger.log_actual("AI", "Processing with enhanced parallel search context...")
    ai_processing_start = time.time()
    
    response = client.models.generate_content(
        model="gemini-2.5-flash",
        contents=prompt
    )
    
    ai_processing_time = time.time() - ai_processing_start
    final_answer = clean_text(response.text)
    
    logger.log_actual("SUCCESS", "Gemini processing completed", 
                     f"{ai_processing_time:.2f}s, input: {len(prompt)} chars, output: {len(final_answer)} chars")
    
    return final_answer

# ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜ í•¨ìˆ˜ (LLM ê¸°ë°˜)
def classify_question(user_input):
    """
    LLM(Gemini)ì„ í™œìš©í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì•„ë˜ ë„¤ ê°€ì§€ ìœ í˜• ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤.
    - 'web_search': ë¬¼í’ˆ ê°œìš”, ìš©ë„, ê¸°ìˆ ê°œë°œ, ë¬´ì—­ë™í–¥, ì‚°ì—…ë™í–¥ ë“±
    - 'hs_classification': HS ì½”ë“œ, í’ˆëª©ë¶„ë¥˜, ê´€ì„¸ ë“±
    - 'hs_manual': HS í•´ì„¤ì„œ ë³¸ë¬¸ ì‹¬ì¸µ ë¶„ì„
    - 'overseas_hs': í•´ì™¸(ë¯¸êµ­/EU) HS ë¶„ë¥˜ ì‚¬ë¡€
    """
    system_prompt = """
ì•„ë˜ëŠ” HS í’ˆëª©ë¶„ë¥˜ ì „ë¬¸ê°€ë¥¼ ìœ„í•œ ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜ ê¸°ì¤€ì…ë‹ˆë‹¤.

ì§ˆë¬¸ ìœ í˜•:
1. "web_search" : "ë‰´ìŠ¤", "ìµœê·¼", "ë™í–¥", "í•´ì™¸", "ì‚°ì—…, ê¸°ìˆ , ë¬´ì—­ë™í–¥" ë“± ì¼ë°˜ ì •ë³´ íƒìƒ‰ì´ í•„ìš”í•œ ê²½ìš°.
2. "hs_classification": HS ì½”ë“œ, í’ˆëª©ë¶„ë¥˜, ê´€ì„¸, ì„¸ìœ¨ ë“± HS ì½”ë“œ ê´€ë ¨ ì •ë³´ê°€ í•„ìš”í•œ ê²½ìš°.
3. "hs_manual": HS í•´ì„¤ì„œ ë³¸ë¬¸ ì‹¬ì¸µ ë¶„ì„ì´ í•„ìš”í•œ ê²½ìš°.
4. "overseas_hs": "ë¯¸êµ­", "í•´ì™¸", "ì™¸êµ­", "US", "America", "EU", "ìœ ëŸ½" ë“± í•´ì™¸ HS ë¶„ë¥˜ ì‚¬ë¡€ê°€ í•„ìš”í•œ ê²½ìš°.
5. "hs_manual_raw": HS ì½”ë“œë§Œ ì…ë ¥í•˜ì—¬ í•´ì„¤ì„œ ì›ë¬¸ì„ ë³´ê³  ì‹¶ì€ ê²½ìš°.

ì•„ë˜ ì‚¬ìš©ì ì§ˆë¬¸ì„ ì½ê³ , ë°˜ë“œì‹œ ìœ„ ë‹¤ì„¯ ê°€ì§€ ì¤‘ í•˜ë‚˜ì˜ ìœ í˜•ë§Œ í•œê¸€ì´ ì•„ë‹Œ ì†Œë¬¸ì ì˜ë¬¸ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
ì§ˆë¬¸: """ + user_input + """\në‹µë³€:"""

    response = client.models.generate_content(
        model="gemini-2.0-flash", # ë˜ëŠ” ìµœì‹  ëª¨ë¸ë¡œ ë³€ê²½ ê°€ëŠ¥
        contents=system_prompt,
        )
    answer = response.text.strip().lower()
    # ê²°ê³¼ê°€ ì •í™•íˆ ë„¤ ê°€ì§€ ì¤‘ í•˜ë‚˜ì¸ì§€ í™•ì¸
    if answer in ["web_search", "hs_classification", "hs_manual", "overseas_hs", "hs_manual_raw"]:
        return answer
    # ì˜ˆì™¸ ì²˜ë¦¬: ë¶„ë¥˜ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’
    return "hs_classification"

# ì§ˆë¬¸ ìœ í˜•ë³„ ì²˜ë¦¬ í•¨ìˆ˜
def handle_web_search(user_input, context, hs_manager):
    # ì›¹ê²€ìƒ‰ ì „ìš© ì»¨í…ìŠ¤íŠ¸ë¡œ ìˆ˜ì •
    web_context = """ë‹¹ì‹ ì€ HS í’ˆëª©ë¶„ë¥˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ìµœì‹  ì›¹ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ì—¬ ë¬¼í’ˆê°œìš”, ìš©ë„, ê¸°ìˆ ê°œë°œ, ë¬´ì—­ë™í–¥, ì‚°ì—…ë™í–¥ ë“±ì˜ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.
êµ­ë‚´ HS ë¶„ë¥˜ ì‚¬ë¡€ê°€ ì•„ë‹Œ ì¼ë°˜ì ì¸ ì‹œì¥ ì •ë³´ì™€ ë™í–¥ì„ ì¤‘ì‹¬ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."""
    
    grounding_tool = types.Tool(google_search=types.GoogleSearch())
    config = types.GenerateContentConfig(tools=[grounding_tool])
    
    prompt = f"{web_context}\n\nì‚¬ìš©ì: {user_input}\n"
    
    response = client.models.generate_content(
        model="gemini-2.5-flash",
        contents=prompt,
        config=config)
    
    return clean_text(response.text)

def handle_hs_classification_cases(user_input, context, hs_manager):
    """êµ­ë‚´ HS ë¶„ë¥˜ ì‚¬ë¡€ ì²˜ë¦¬ (ê·¸ë£¹ë³„ Gemini + Head Agent)"""
    # 5ê°œ ê·¸ë£¹ë³„ë¡œ ê°ê° Geminiì— ë¶€ë¶„ ë‹µë³€ ìš”ì²­
    group_answers = []
    for i in range(5):  # 3 â†’ 5ë¡œ ë³€ê²½
        relevant = hs_manager.get_domestic_context_group(user_input, i)
        prompt = f"{context}\n\nê´€ë ¨ ë°ì´í„° (êµ­ë‚´ ê´€ì„¸ì²­, ê·¸ë£¹{i+1}):\n{relevant}\n\nì‚¬ìš©ì: {user_input}\n"
        response = client.models.generate_content(
            model="gemini-2.0-flash",
            contents=prompt
        )
        group_answers.append(clean_text(response.text))

    # Head Agentê°€ 5ê°œ ë¶€ë¶„ ë‹µë³€ì„ ì·¨í•©í•˜ì—¬ ìµœì¢… ë‹µë³€ ìƒì„±
    head_prompt = f"{context}\n\nì•„ë˜ëŠ” êµ­ë‚´ HS ë¶„ë¥˜ ì‚¬ë¡€ ë°ì´í„° 5ê°œ ê·¸ë£¹ë³„ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤. ê° ê·¸ë£¹ì˜ ë‹µë³€ì„ ì¢…í•©í•˜ì—¬ ìµœì¢… ì „ë¬¸ê°€ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.\n\n"
    for idx, ans in enumerate(group_answers):
        head_prompt += f"[ê·¸ë£¹{idx+1} ë‹µë³€]\n{ans}\n\n"
    head_prompt += f"\nì‚¬ìš©ì: {user_input}\n"
    head_response = client.models.generate_content(
        model="gemini-2.5-flash",
        contents=head_prompt
    )
    return clean_text(head_response.text)

def handle_hs_manual(user_input, context, hs_manager):
    # ì˜ˆ: HS í•´ì„¤ì„œ ë¶„ì„ ì „ìš© ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
    manual_context = context + "\n(ì‹¬ì¸µ í•´ì„¤ì„œ ë¶„ì„ ëª¨ë“œ)"
    hs_codes = extract_hs_codes(user_input)
    explanations = get_hs_explanations(hs_codes) if hs_codes else ""
    prompt = f"{manual_context}\n\nê´€ë ¨ ë°ì´í„°:\n{explanations}\n\nì‚¬ìš©ì: {user_input}\n"
    # client.models.generate_content ì‚¬ìš©
    response = client.models.generate_content(
        model="gemini-2.5-flash", # ëª¨ë¸ëª… ë‹¨ìˆœí™”
        contents=prompt
    )
    return clean_text(response.text)

def handle_overseas_hs(user_input, context, hs_manager):
    """í•´ì™¸ HS ë¶„ë¥˜ ì‚¬ë¡€ ì²˜ë¦¬ (ê·¸ë£¹ë³„ Gemini + Head Agent)"""
    overseas_context = context + "\n(í•´ì™¸ HS ë¶„ë¥˜ ì‚¬ë¡€ ë¶„ì„ ëª¨ë“œ)"
    
    # 5ê°œ ê·¸ë£¹ë³„ë¡œ ê°ê° Geminiì— ë¶€ë¶„ ë‹µë³€ ìš”ì²­
    group_answers = []
    for i in range(5):
        relevant = hs_manager.get_overseas_context_group(user_input, i)
        prompt = f"{overseas_context}\n\nê´€ë ¨ ë°ì´í„° (í•´ì™¸ ê´€ì„¸ì²­, ê·¸ë£¹{i+1}):\n{relevant}\n\nì‚¬ìš©ì: {user_input}\n"
        response = client.models.generate_content(
            model="gemini-2.0-flash",
            contents=prompt
        )
        group_answers.append(clean_text(response.text))

    # Head Agentê°€ 5ê°œ ë¶€ë¶„ ë‹µë³€ì„ ì·¨í•©í•˜ì—¬ ìµœì¢… ë‹µë³€ ìƒì„±
    head_prompt = f"{overseas_context}\n\nì•„ë˜ëŠ” í•´ì™¸ HS ë¶„ë¥˜ ì‚¬ë¡€ ë°ì´í„° 5ê°œ ê·¸ë£¹ë³„ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤. ê° ê·¸ë£¹ì˜ ë‹µë³€ì„ ì¢…í•©í•˜ì—¬ ìµœì¢… ì „ë¬¸ê°€ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.\n\n"
    for idx, ans in enumerate(group_answers):
        head_prompt += f"[ê·¸ë£¹{idx+1} ë‹µë³€]\n{ans}\n\n"
    head_prompt += f"\nì‚¬ìš©ì: {user_input}\n"
    head_response = client.models.generate_content(
        model="gemini-2.5-flash",
        contents=head_prompt
    )
    return clean_text(head_response.text)