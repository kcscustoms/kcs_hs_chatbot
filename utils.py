import json
import re
import os
import requests
from typing import Dict, List, Any
from collections import defaultdict
from google import genai
from google.genai import types
from dotenv import load_dotenv



# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (.env íŒŒì¼ì—ì„œ API í‚¤ ë“± ì„¤ì •ê°’ ë¡œë“œ)
load_dotenv()
GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')
client = genai.Client(api_key=GOOGLE_API_KEY)

class HSDataManager:
    """
    HS ì½”ë“œ ê´€ë ¨ ë°ì´í„°ë¥¼ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤
    - HS ë¶„ë¥˜ ì‚¬ë¡€, ìœ„ì›íšŒ ê²°ì •, í˜‘ì˜íšŒ ê²°ì • ë“±ì˜ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ê´€ë¦¬
    - í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ ê¸°ëŠ¥ ì œê³µ
    - ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ìƒì„± ê¸°ëŠ¥ ì œê³µ
    """
    
    def __init__(self):
        """HSDataManager ì´ˆê¸°í™”"""
        self.data = {}  # ëª¨ë“  HS ê´€ë ¨ ë°ì´í„°ë¥¼ ì €ì¥í•˜ëŠ” ë”•ì…”ë„ˆë¦¬
        self.search_index = defaultdict(list)  # í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ì„ ìœ„í•œ ì¸ë±ìŠ¤
        self.load_all_data()  # ëª¨ë“  ë°ì´í„° íŒŒì¼ ë¡œë“œ
        self.build_search_index()  # ê²€ìƒ‰ ì¸ë±ìŠ¤ êµ¬ì¶•
    
    def load_all_data(self):
        """
        ëª¨ë“  HS ë°ì´í„° íŒŒì¼ì„ ë¡œë“œí•˜ëŠ” ë©”ì„œë“œ
        - HSë¶„ë¥˜ì‚¬ë¡€_part1~10.json íŒŒì¼ ë¡œë“œ
        - HSìœ„ì›íšŒ.json, HSí˜‘ì˜íšŒ.json íŒŒì¼ ë¡œë“œ
        - hs_classification_data_us.json íŒŒì¼ ë¡œë“œ (ë¯¸êµ­ ê´€ì„¸ì²­ í’ˆëª©ë¶„ë¥˜ ì‚¬ë¡€)
        - hs_classification_data_eu.json íŒŒì¼ ë¡œë“œ (EU ê´€ì„¸ì²­ í’ˆëª©ë¶„ë¥˜ ì‚¬ë¡€)
        """
        # HSë¶„ë¥˜ì‚¬ë¡€ íŒŒíŠ¸ ë¡œë“œ (1~10)
        for i in range(1, 11):
            try:
                with open(f'knowledge/HSë¶„ë¥˜ì‚¬ë¡€_part{i}.json', 'r', encoding='utf-8') as f:
                    self.data[f'HSë¶„ë¥˜ì‚¬ë¡€_part{i}'] = json.load(f)
            except FileNotFoundError:
                print(f'Warning: HSë¶„ë¥˜ì‚¬ë¡€_part{i}.json not found')
        
        # ê¸°íƒ€ JSON íŒŒì¼ ë¡œë“œ (ìœ„ì›íšŒ, í˜‘ì˜íšŒ ê²°ì •)
        other_files = ['knowledge/HSìœ„ì›íšŒ.json', 'knowledge/HSí˜‘ì˜íšŒ.json']
        for file in other_files:
            try:
                with open(file, 'r', encoding='utf-8') as f:
                    self.data[file.replace('.json', '')] = json.load(f)
            except FileNotFoundError:
                print(f'Warning: {file} not found')
        
        # ë¯¸êµ­ ê´€ì„¸ì²­ í’ˆëª©ë¶„ë¥˜ ì‚¬ë¡€ ë¡œë“œ
        try:
            with open('knowledge/hs_classification_data_us.json', 'r', encoding='utf-8') as f:
                self.data['hs_classification_data_us'] = json.load(f)
        except FileNotFoundError:
            print('Warning: hs_classification_data_us.json not found')
        
        # EU ê´€ì„¸ì²­ í’ˆëª©ë¶„ë¥˜ ì‚¬ë¡€ ë¡œë“œ
        try:
            with open('knowledge/hs_classification_data_eu.json', 'r', encoding='utf-8') as f:
                self.data['hs_classification_data_eu'] = json.load(f)
        except FileNotFoundError:
            print('Warning: hs_classification_data_eu.json not found')
    
    def build_search_index(self):
        """
        ê²€ìƒ‰ ì¸ë±ìŠ¤ êµ¬ì¶• ë©”ì„œë“œ
        - ê° ë°ì´í„° í•­ëª©ì—ì„œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œ
        - ì¶”ì¶œëœ í‚¤ì›Œë“œë¥¼ ì¸ë±ìŠ¤ì— ì €ì¥í•˜ì—¬ ë¹ ë¥¸ ê²€ìƒ‰ ê°€ëŠ¥
        """
        for source, items in self.data.items():
            for item in items:
                # í’ˆëª©ëª…ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
                keywords = self._extract_keywords(str(item))
                # ê° í‚¤ì›Œë“œì— ëŒ€í•´ í•´ë‹¹ ì•„ì´í…œ ì°¸ì¡° ì €ì¥
                for keyword in keywords:
                    self.search_index[keyword].append((source, item))
    
    def _extract_keywords(self, text: str) -> List[str]:
        """
        í…ìŠ¤íŠ¸ì—ì„œ ì˜ë¯¸ìˆëŠ” í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ëŠ” ë‚´ë¶€ ë©”ì„œë“œ
        Args:
            text: í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•  í…ìŠ¤íŠ¸
        Returns:
            ì¶”ì¶œëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        """
        # íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ê³µë°± ê¸°ì¤€ ë¶„ë¦¬
        words = re.sub(r'[^\w\s]', ' ', text).split()
        # ì¤‘ë³µ ì œê±° ë° ê¸¸ì´ 2 ì´ìƒì¸ ë‹¨ì–´ë§Œ ì„ íƒ
        return list(set(word for word in words if len(word) >= 2))
    
    def search(self, query: str, max_results: int = 5) -> List[Dict[str, Any]]:
        """
        ì¿¼ë¦¬ì™€ ê´€ë ¨ëœ ê°€ì¥ ì—°ê´€ì„± ë†’ì€ í•­ëª©ë“¤ì„ ê²€ìƒ‰í•˜ëŠ” ë©”ì„œë“œ
        Args:
            query: ê²€ìƒ‰í•  ì¿¼ë¦¬ ë¬¸ìì—´
            max_results: ë°˜í™˜í•  ìµœëŒ€ ê²°ê³¼ ìˆ˜ (ê¸°ë³¸ê°’: 5)
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ (ì¶œì²˜ì™€ í•­ëª© ì •ë³´ í¬í•¨)
        """
        query_keywords = self._extract_keywords(query)
        results = defaultdict(int)
        
        # ê° í‚¤ì›Œë“œì— ëŒ€í•´ ë§¤ì¹­ë˜ëŠ” í•­ëª© ì°¾ê¸°
        for keyword in query_keywords:
            for source, item in self.search_index.get(keyword, []):
                # ê°€ì¤‘ì¹˜ ê³„ì‚° (í‚¤ì›Œë“œ ë§¤ì¹­ íšŸìˆ˜ ê¸°ë°˜)
                results[(source, str(item))] += 1
        
        # ê°€ì¤‘ì¹˜ ê¸°ì¤€ ì •ë ¬
        sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)
        
        # ìƒìœ„ ê²°ê³¼ë§Œ ë°˜í™˜
        return [
            {'source': source, 'item': eval(item_str)}
            for (source, item_str), _ in sorted_results[:max_results]
        ]
    
    def search_domestic_group(self, query: str, group_idx: int, max_results: int = 3) -> List[Dict[str, Any]]:
        """êµ­ë‚´ HS ë¶„ë¥˜ ë°ì´í„° ê·¸ë£¹ë³„ ê²€ìƒ‰ ë©”ì„œë“œ"""
        query_keywords = self._extract_keywords(query)
        results = defaultdict(int)

        # ê·¸ë£¹ë³„ ë°ì´í„° ì†ŒìŠ¤ ì •ì˜ (5ê°œ ê·¸ë£¹)
        group_sources = [
            ['HSë¶„ë¥˜ì‚¬ë¡€_part1', 'HSë¶„ë¥˜ì‚¬ë¡€_part2'],  # ê·¸ë£¹1
            ['HSë¶„ë¥˜ì‚¬ë¡€_part3', 'HSë¶„ë¥˜ì‚¬ë¡€_part4'],  # ê·¸ë£¹2
            ['HSë¶„ë¥˜ì‚¬ë¡€_part5', 'HSë¶„ë¥˜ì‚¬ë¡€_part6'],  # ê·¸ë£¹3
            ['HSë¶„ë¥˜ì‚¬ë¡€_part7', 'HSë¶„ë¥˜ì‚¬ë¡€_part8'],  # ê·¸ë£¹4
            ['HSë¶„ë¥˜ì‚¬ë¡€_part9', 'HSë¶„ë¥˜ì‚¬ë¡€_part10', 'knowledge/HSìœ„ì›íšŒ', 'knowledge/HSí˜‘ì˜íšŒ']  # ê·¸ë£¹5
        ]
        sources = group_sources[group_idx]

        for keyword in query_keywords:
            for source, item in self.search_index.get(keyword, []):
                if source in sources:
                    results[(source, str(item))] += 1

        sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)
        return [
            {'source': source, 'item': eval(item_str)}
            for (source, item_str), _ in sorted_results[:max_results]
        ]

    def get_domestic_context_group(self, query: str, group_idx: int) -> str:
        """êµ­ë‚´ HS ë¶„ë¥˜ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸(ê·¸ë£¹ë³„)ë¥¼ ìƒì„±í•˜ëŠ” ë©”ì„œë“œ"""
        results = self.search_domestic_group(query, group_idx)
        context = []
        for result in results:
            context.append(f"ì¶œì²˜: {result['source']} (êµ­ë‚´ ê´€ì„¸ì²­)\ní•­ëª©: {json.dumps(result['item'], ensure_ascii=False)}")
        return "\n\n".join(context)

    def search_overseas_group(self, query: str, group_idx: int, max_results: int = 3) -> List[Dict[str, Any]]:
        """í•´ì™¸ HS ë¶„ë¥˜ ë°ì´í„° ê·¸ë£¹ë³„ ê²€ìƒ‰ ë©”ì„œë“œ"""
        query_keywords = self._extract_keywords(query)
        results = defaultdict(int)
        
        # í•´ì™¸ ë°ì´í„°ë¥¼ ê·¸ë£¹ë³„ë¡œ ë¶„í•  ì²˜ë¦¬
        if group_idx < 3:  # ê·¸ë£¹ 0,1,2ëŠ” ë¯¸êµ­ ë°ì´í„°
            target_source = 'hs_classification_data_us'
            # ë¯¸êµ­ ë°ì´í„°ë¥¼ 3ë“±ë¶„
            us_data = self.data.get(target_source, [])
            chunk_size = len(us_data) // 3
            start_idx = group_idx * chunk_size
            end_idx = start_idx + chunk_size if group_idx < 2 else len(us_data)
            target_items = us_data[start_idx:end_idx]
        else:  # ê·¸ë£¹ 3,4ëŠ” EU ë°ì´í„°
            target_source = 'hs_classification_data_eu'
            # EU ë°ì´í„°ë¥¼ 2ë“±ë¶„
            eu_data = self.data.get(target_source, [])
            chunk_size = len(eu_data) // 2
            eu_group_idx = group_idx - 3  # 0 or 1
            start_idx = eu_group_idx * chunk_size
            end_idx = start_idx + chunk_size if eu_group_idx < 1 else len(eu_data)
            target_items = eu_data[start_idx:end_idx]
        
        # í•´ë‹¹ ê·¸ë£¹ ë°ì´í„°ì—ì„œë§Œ ê²€ìƒ‰
        for keyword in query_keywords:
            for source, item in self.search_index.get(keyword, []):
                if source == target_source and item in target_items:
                    results[(source, str(item))] += 1
        
        sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)
        return [
            {'source': source, 'item': eval(item_str)}
            for (source, item_str), _ in sorted_results[:max_results]
        ]

    def get_overseas_context_group(self, query: str, group_idx: int) -> str:
        """í•´ì™¸ HS ë¶„ë¥˜ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸(ê·¸ë£¹ë³„)ë¥¼ ìƒì„±í•˜ëŠ” ë©”ì„œë“œ"""
        results = self.search_overseas_group(query, group_idx)
        context = []
        
        for result in results:
            # ì¶œì²˜ì— ë”°ë¼ êµ­ê°€ êµ¬ë¶„
            if result['source'] == 'hs_classification_data_us':
                country = "ë¯¸êµ­ ê´€ì„¸ì²­"
            elif result['source'] == 'hs_classification_data_eu':
                country = "EU ê´€ì„¸ì²­"
            else:
                country = "í•´ì™¸ ê´€ì„¸ì²­"
                
            context.append(f"ì¶œì²˜: {result['source']} ({country})\ní•­ëª©: {json.dumps(result['item'], ensure_ascii=False)}")
        
        return "\n\n".join(context)
    
    def search_domestic(self, query: str, max_results: int = 5) -> List[Dict[str, Any]]:
        """êµ­ë‚´ HS ë¶„ë¥˜ ë°ì´í„°ì—ì„œë§Œ ê²€ìƒ‰í•˜ëŠ” ë©”ì„œë“œ"""
        query_keywords = self._extract_keywords(query)
        results = defaultdict(int)
        
        # êµ­ë‚´ ë°ì´í„° ì†ŒìŠ¤ë§Œ í•„í„°ë§
        domestic_sources = [
            'HSë¶„ë¥˜ì‚¬ë¡€_part1', 'HSë¶„ë¥˜ì‚¬ë¡€_part2', 'HSë¶„ë¥˜ì‚¬ë¡€_part3', 'HSë¶„ë¥˜ì‚¬ë¡€_part4', 'HSë¶„ë¥˜ì‚¬ë¡€_part5',
            'HSë¶„ë¥˜ì‚¬ë¡€_part6', 'HSë¶„ë¥˜ì‚¬ë¡€_part7', 'HSë¶„ë¥˜ì‚¬ë¡€_part8', 'HSë¶„ë¥˜ì‚¬ë¡€_part9', 'HSë¶„ë¥˜ì‚¬ë¡€_part10',
            'knowledge/HSìœ„ì›íšŒ', 'knowledge/HSí˜‘ì˜íšŒ'
        ]
        
        for keyword in query_keywords:
            for source, item in self.search_index.get(keyword, []):
                # êµ­ë‚´ ë°ì´í„° ì†ŒìŠ¤ë§Œ í¬í•¨
                if source in domestic_sources:
                    results[(source, str(item))] += 1
        
        sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)
        
        return [
            {'source': source, 'item': eval(item_str)}
            for (source, item_str), _ in sorted_results[:max_results]
        ]
    
    def get_domestic_context(self, query: str) -> str:
        """êµ­ë‚´ HS ë¶„ë¥˜ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ë©”ì„œë“œ"""
        results = self.search_domestic(query)
        context = []
        
        for result in results:
            context.append(f"ì¶œì²˜: {result['source']} (êµ­ë‚´ ê´€ì„¸ì²­)\ní•­ëª©: {json.dumps(result['item'], ensure_ascii=False)}")
        
        return "\n\n".join(context)
    
    def search_overseas_improved(self, query: str, max_results: int = 5) -> List[Dict[str, Any]]:
        """í•´ì™¸ HS ë¶„ë¥˜ ë°ì´í„°ì—ì„œë§Œ ê²€ìƒ‰í•˜ëŠ” ê°œì„ ëœ ë©”ì„œë“œ (search_index í™œìš©)"""
        query_keywords = self._extract_keywords(query)
        results = defaultdict(int)
        
        # í•´ì™¸ ë°ì´í„° ì†ŒìŠ¤ë§Œ í•„í„°ë§
        overseas_sources = ['hs_classification_data_us', 'hs_classification_data_eu']
        
        for keyword in query_keywords:
            for source, item in self.search_index.get(keyword, []):
                # í•´ì™¸ ë°ì´í„° ì†ŒìŠ¤ë§Œ í¬í•¨
                if source in overseas_sources:
                    results[(source, str(item))] += 1
        
        sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)
        
        return [
            {'source': source, 'item': eval(item_str)}
            for (source, item_str), _ in sorted_results[:max_results]
        ]
    
    def get_domestic_context(self, query: str) -> str:
        """êµ­ë‚´ HS ë¶„ë¥˜ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ë©”ì„œë“œ"""
        results = self.search_domestic(query)
        context = []
        
        for result in results:
            context.append(f"ì¶œì²˜: {result['source']} (êµ­ë‚´ ê´€ì„¸ì²­)\ní•­ëª©: {json.dumps(result['item'], ensure_ascii=False)}")
        
        return "\n\n".join(context)


    def get_relevant_context(self, query: str) -> str:
        """
        ì¿¼ë¦¬ì— ê´€ë ¨ëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ë©”ì„œë“œ
        Args:
            query: ì»¨í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•  ì¿¼ë¦¬ ë¬¸ìì—´
        Returns:
            ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ (ì¶œì²˜ì™€ í•­ëª© ì •ë³´ í¬í•¨)
        """
        results = self.search(query)
        context = []
        
        for result in results:
            context.append(f"ì¶œì²˜: {result['source']}\ní•­ëª©: {json.dumps(result['item'], ensure_ascii=False)}")
        
        return "\n\n".join(context)
    
    def get_overseas_context_improved(self, query: str) -> str:
        """í•´ì™¸ HS ë¶„ë¥˜ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ê°œì„ ëœ ë©”ì„œë“œ"""
        results = self.search_overseas_improved(query)
        context = []
        
        for result in results:
            # ì¶œì²˜ì— ë”°ë¼ êµ­ê°€ êµ¬ë¶„
            if result['source'] == 'hs_classification_data_us':
                country = "ë¯¸êµ­ ê´€ì„¸ì²­"
            elif result['source'] == 'hs_classification_data_eu':
                country = "EU ê´€ì„¸ì²­"
            else:
                country = "í•´ì™¸ ê´€ì„¸ì²­"
                
            context.append(f"ì¶œì²˜: {result['source']} ({country})\ní•­ëª©: {json.dumps(result['item'], ensure_ascii=False)}")
        
        return "\n\n".join(context)

# HTML íƒœê·¸ ì œê±° ë° í…ìŠ¤íŠ¸ ì •ì œ í•¨ìˆ˜
def clean_text(text):
    # HTML íƒœê·¸ ì œê±° (ë” ì—„ê²©í•œ ì •ê·œì‹ íŒ¨í„´ ì‚¬ìš©)
    text = re.sub(r'<[^>]+>', '', text)  # ëª¨ë“  HTML íƒœê·¸ ì œê±°
    text = re.sub(r'\s*</div>\s*$', '', text)  # ëì— ìˆëŠ” </div> íƒœê·¸ ì œê±°
    return text.strip()

# HS ì½”ë“œ ì¶”ì¶œ íŒ¨í„´ ì •ì˜ ë° í•¨ìˆ˜
# ë” ìœ ì—°í•œ HS ì½”ë“œ ì¶”ì¶œ íŒ¨í„´
HS_PATTERN = re.compile(
    r'(?:HS\s*)?(\d{4}(?:[.-]?\d{2}(?:[.-]?\d{2}(?:[.-]?\d{2})?)?)?)',
    flags=re.IGNORECASE
)

def extract_hs_codes(text):
    """
    ì—¬ëŸ¬ HS ì½”ë“œë¥¼ ì¶”ì¶œí•˜ê³ , ì¤‘ë³µ ì œê±° ë° ìˆ«ìë§Œ ë‚¨ê²¨ í‘œì¤€í™”
    ê°œì„ ì‚¬í•­:
    - ë‹¨ì–´ ê²½ê³„(\b) ì œê±°ë¡œ ë” ìœ ì—°í•œ ë§¤ì¹­
    - ìˆ«ìë§Œ ìˆëŠ” ê²½ìš°ë„ ì²˜ë¦¬ ê°€ëŠ¥
    - ìµœì†Œ 4ìë¦¬ ìˆ«ì ì²´í¬ ì¶”ê°€
    """
    matches = HS_PATTERN.findall(text)
    hs_codes = []
    
    for raw in matches:
        # ìˆ«ìë§Œ ë‚¨ê¸°ê¸°
        code = re.sub(r'\D', '', raw)
        # ìµœì†Œ 4ìë¦¬ì´ê³  ì¤‘ë³µì´ ì•„ë‹Œ ê²½ìš°ë§Œ ì¶”ê°€
        if len(code) >= 4 and code not in hs_codes:
            hs_codes.append(code)
    
    # ë§Œì•½ ìœ„ íŒ¨í„´ìœ¼ë¡œ ì°¾ì§€ ëª»í•˜ê³ , ì…ë ¥ì´ 4ìë¦¬ ì´ìƒì˜ ìˆ«ìë¡œë§Œ êµ¬ì„±ëœ ê²½ìš°
    if not hs_codes:
        # ìˆœìˆ˜ ìˆ«ìë§Œ ìˆëŠ” ê²½ìš° ì²´í¬
        numbers_only = re.findall(r'\d{4,}', text)
        for num in numbers_only:
            if num not in hs_codes:
                hs_codes.append(num)
    
    return hs_codes

def extract_and_store_text(json_file):
    """JSON íŒŒì¼ì—ì„œ head1ê³¼ textë¥¼ ì¶”ì¶œí•˜ì—¬ ë³€ìˆ˜ì— ì €ì¥"""
    try:
        # JSON íŒŒì¼ ì½ê¸°
        with open(json_file, 'r', encoding='utf-8') as file:
            data = json.load(file)
        
        # ë°ì´í„°ë¥¼ ë³€ìˆ˜ì— ì €ì¥
        extracted_data = []
        for item in data:
            head1 = item.get('head1', '')
            text = item.get('text', '')
            if head1 or text:
                extracted_data.append(f"{head1}\n{text}")
        
        return extracted_data
    except Exception as e:
        print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []

# í†µì¹™ ë°ì´í„° ë¡œë“œ (ì¬ì‚¬ìš©ì„ ìœ„í•œ ì „ì—­ ë³€ìˆ˜)
general_explanation = extract_and_store_text('knowledge/í†µì¹™_grouped.json')

def lookup_hscode(hs_code, json_file):
    """HS ì½”ë“œì— ëŒ€í•œ í•´ì„¤ ì •ë³´ë¥¼ ì¡°íšŒí•˜ëŠ” í•¨ìˆ˜"""
    try:
        with open(json_file, 'r', encoding='utf-8') as file:
            data = json.load(file)
        
        # ê° ì„¤ëª… ìœ í˜•ë³„ ì´ˆê¸°ê°’ ì„¤ì •
        part_explanation = {"text": "í•´ë‹¹ ë¶€ì— ëŒ€í•œ ì„¤ëª…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
        chapter_explanation = {"text": "í•´ë‹¹ ë¥˜ì— ëŒ€í•œ ì„¤ëª…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
        sub_explanation = {"text": "í•´ë‹¹ í˜¸ì— ëŒ€í•œ ì„¤ëª…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

        # 1) ë¥˜(é¡) key: "ì œ00ë¥˜"
        chapter_key = f"ì œ{int(hs_code[:2])}ë¥˜"
        chapter_explanation = next((g for g in data if g.get('header2') == chapter_key), chapter_explanation)

        # 2) í˜¸ key: "00.00"
        sub_key = f"{hs_code[:2]}.{hs_code[2:]}"
        sub_explanation = next((g for g in data if g.get('header2') == sub_key), sub_explanation)

        # 3) ë¶€(éƒ¨) key: "ì œ00ë¶€"
        part_key = chapter_explanation.get('header1') if chapter_explanation else None
        part_explanation = next((g for g in data if (g.get('header1') == part_key)&(re.sub(r'ì œ\s*(\d+)\s*ë¶€', r'ì œ\1ë¶€', g.get('header1')) == part_key)), None)
        
        return part_explanation, chapter_explanation, sub_explanation
    
    except Exception as e:
        print(f"HS ì½”ë“œ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        return ({"text": "ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}, {"text": "ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}, {"text": "ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."})

def get_hs_explanations(hs_codes):
    """ì—¬ëŸ¬ HS ì½”ë“œì— ëŒ€í•œ í•´ì„¤ì„ ì·¨í•©í•˜ëŠ” í•¨ìˆ˜ (ë§ˆí¬ë‹¤ìš´ í˜•ì‹)"""
    all_explanations = ""
    for hs_code in hs_codes:
        explanation, type_explanation, number_explanation = lookup_hscode(hs_code, 'knowledge/grouped_11_end.json')

        if explanation and type_explanation and number_explanation:
            all_explanations += f"\n\n# HS ì½”ë“œ {hs_code} í•´ì„¤\n\n"
            all_explanations += f"## ğŸ“‹ í•´ì„¤ì„œ í†µì¹™\n\n"
            
            # í†µì¹™ ë‚´ìš©ì„ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì •ë¦¬
            if general_explanation:
                for i, rule in enumerate(general_explanation[:5], 1):  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
                    all_explanations += f"### í†µì¹™ {i}\n{rule}\n\n"
            
            all_explanations += f"## ğŸ“‚ ë¶€(éƒ¨) í•´ì„¤\n\n{explanation['text']}\n\n"
            all_explanations += f"## ğŸ“š ë¥˜(é¡) í•´ì„¤\n\n{type_explanation['text']}\n\n"
            all_explanations += f"## ğŸ“ í˜¸(è™Ÿ) í•´ì„¤\n\n{number_explanation['text']}\n\n"
            all_explanations += "---\n"  # êµ¬ë¶„ì„  ì¶”ê°€
    
    return all_explanations

# ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜ í•¨ìˆ˜ (LLM ê¸°ë°˜)
def classify_question(user_input):
    """
    LLM(Gemini)ì„ í™œìš©í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì•„ë˜ ë„¤ ê°€ì§€ ìœ í˜• ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤.
    - 'web_search': ë¬¼í’ˆ ê°œìš”, ìš©ë„, ê¸°ìˆ ê°œë°œ, ë¬´ì—­ë™í–¥, ì‚°ì—…ë™í–¥ ë“±
    - 'hs_classification': HS ì½”ë“œ, í’ˆëª©ë¶„ë¥˜, ê´€ì„¸ ë“±
    - 'hs_manual': HS í•´ì„¤ì„œ ë³¸ë¬¸ ì‹¬ì¸µ ë¶„ì„
    - 'overseas_hs': í•´ì™¸(ë¯¸êµ­/EU) HS ë¶„ë¥˜ ì‚¬ë¡€
    """
    system_prompt = """
ì•„ë˜ëŠ” HS í’ˆëª©ë¶„ë¥˜ ì „ë¬¸ê°€ë¥¼ ìœ„í•œ ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜ ê¸°ì¤€ì…ë‹ˆë‹¤.

ì§ˆë¬¸ ìœ í˜•:
1. "web_search" : "ë‰´ìŠ¤", "ìµœê·¼", "ë™í–¥", "í•´ì™¸", "ì‚°ì—…, ê¸°ìˆ , ë¬´ì—­ë™í–¥" ë“± ì¼ë°˜ ì •ë³´ íƒìƒ‰ì´ í•„ìš”í•œ ê²½ìš°.
2. "hs_classification": HS ì½”ë“œ, í’ˆëª©ë¶„ë¥˜, ê´€ì„¸, ì„¸ìœ¨ ë“± HS ì½”ë“œ ê´€ë ¨ ì •ë³´ê°€ í•„ìš”í•œ ê²½ìš°.
3. "hs_manual": HS í•´ì„¤ì„œ ë³¸ë¬¸ ì‹¬ì¸µ ë¶„ì„ì´ í•„ìš”í•œ ê²½ìš°.
4. "overseas_hs": "ë¯¸êµ­", "í•´ì™¸", "ì™¸êµ­", "US", "America", "EU", "ìœ ëŸ½" ë“± í•´ì™¸ HS ë¶„ë¥˜ ì‚¬ë¡€ê°€ í•„ìš”í•œ ê²½ìš°.
5. "hs_manual_raw": HS ì½”ë“œë§Œ ì…ë ¥í•˜ì—¬ í•´ì„¤ì„œ ì›ë¬¸ì„ ë³´ê³  ì‹¶ì€ ê²½ìš°.

ì•„ë˜ ì‚¬ìš©ì ì§ˆë¬¸ì„ ì½ê³ , ë°˜ë“œì‹œ ìœ„ ë‹¤ì„¯ ê°€ì§€ ì¤‘ í•˜ë‚˜ì˜ ìœ í˜•ë§Œ í•œê¸€ì´ ì•„ë‹Œ ì†Œë¬¸ì ì˜ë¬¸ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
ì§ˆë¬¸: """ + user_input + """\në‹µë³€:"""

    response = client.models.generate_content(
        model="gemini-2.0-flash", # ë˜ëŠ” ìµœì‹  ëª¨ë¸ë¡œ ë³€ê²½ ê°€ëŠ¥
        contents=system_prompt,
        )
    answer = response.text.strip().lower()
    # ê²°ê³¼ê°€ ì •í™•íˆ ë„¤ ê°€ì§€ ì¤‘ í•˜ë‚˜ì¸ì§€ í™•ì¸
    if answer in ["web_search", "hs_classification", "hs_manual", "overseas_hs", "hs_manual_raw"]:
        return answer
    # ì˜ˆì™¸ ì²˜ë¦¬: ë¶„ë¥˜ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’
    return "hs_classification"

# ì§ˆë¬¸ ìœ í˜•ë³„ ì²˜ë¦¬ í•¨ìˆ˜
def handle_web_search(user_input, context, hs_manager):
    # ì›¹ê²€ìƒ‰ ì „ìš© ì»¨í…ìŠ¤íŠ¸ë¡œ ìˆ˜ì •
    web_context = """ë‹¹ì‹ ì€ HS í’ˆëª©ë¶„ë¥˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ìµœì‹  ì›¹ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ì—¬ ë¬¼í’ˆê°œìš”, ìš©ë„, ê¸°ìˆ ê°œë°œ, ë¬´ì—­ë™í–¥, ì‚°ì—…ë™í–¥ ë“±ì˜ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.
êµ­ë‚´ HS ë¶„ë¥˜ ì‚¬ë¡€ê°€ ì•„ë‹Œ ì¼ë°˜ì ì¸ ì‹œì¥ ì •ë³´ì™€ ë™í–¥ì„ ì¤‘ì‹¬ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."""
    
    grounding_tool = types.Tool(google_search=types.GoogleSearch())
    config = types.GenerateContentConfig(tools=[grounding_tool])
    
    prompt = f"{web_context}\n\nì‚¬ìš©ì: {user_input}\n"
    
    response = client.models.generate_content(
        model="gemini-2.5-flash",
        contents=prompt,
        config=config)
    
    return clean_text(response.text)

def handle_hs_classification_cases(user_input, context, hs_manager):
    """êµ­ë‚´ HS ë¶„ë¥˜ ì‚¬ë¡€ ì²˜ë¦¬ (ê·¸ë£¹ë³„ Gemini + Head Agent)"""
    # 5ê°œ ê·¸ë£¹ë³„ë¡œ ê°ê° Geminiì— ë¶€ë¶„ ë‹µë³€ ìš”ì²­
    group_answers = []
    for i in range(5):  # 3 â†’ 5ë¡œ ë³€ê²½
        relevant = hs_manager.get_domestic_context_group(user_input, i)
        prompt = f"{context}\n\nê´€ë ¨ ë°ì´í„° (êµ­ë‚´ ê´€ì„¸ì²­, ê·¸ë£¹{i+1}):\n{relevant}\n\nì‚¬ìš©ì: {user_input}\n"
        response = client.models.generate_content(
            model="gemini-2.0-flash",
            contents=prompt
        )
        group_answers.append(clean_text(response.text))

    # Head Agentê°€ 5ê°œ ë¶€ë¶„ ë‹µë³€ì„ ì·¨í•©í•˜ì—¬ ìµœì¢… ë‹µë³€ ìƒì„±
    head_prompt = f"{context}\n\nì•„ë˜ëŠ” êµ­ë‚´ HS ë¶„ë¥˜ ì‚¬ë¡€ ë°ì´í„° 5ê°œ ê·¸ë£¹ë³„ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤. ê° ê·¸ë£¹ì˜ ë‹µë³€ì„ ì¢…í•©í•˜ì—¬ ìµœì¢… ì „ë¬¸ê°€ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.\n\n"
    for idx, ans in enumerate(group_answers):
        head_prompt += f"[ê·¸ë£¹{idx+1} ë‹µë³€]\n{ans}\n\n"
    head_prompt += f"\nì‚¬ìš©ì: {user_input}\n"
    head_response = client.models.generate_content(
        model="gemini-2.5-flash",
        contents=head_prompt
    )
    return clean_text(head_response.text)

def handle_hs_manual(user_input, context, hs_manager):
    # ì˜ˆ: HS í•´ì„¤ì„œ ë¶„ì„ ì „ìš© ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
    manual_context = context + "\n(ì‹¬ì¸µ í•´ì„¤ì„œ ë¶„ì„ ëª¨ë“œ)"
    hs_codes = extract_hs_codes(user_input)
    explanations = get_hs_explanations(hs_codes) if hs_codes else ""
    prompt = f"{manual_context}\n\nê´€ë ¨ ë°ì´í„°:\n{explanations}\n\nì‚¬ìš©ì: {user_input}\n"
    # client.models.generate_content ì‚¬ìš©
    response = client.models.generate_content(
        model="gemini-2.5-flash", # ëª¨ë¸ëª… ë‹¨ìˆœí™”
        contents=prompt
    )
    return clean_text(response.text)

def handle_overseas_hs(user_input, context, hs_manager):
    """í•´ì™¸ HS ë¶„ë¥˜ ì‚¬ë¡€ ì²˜ë¦¬ (ê·¸ë£¹ë³„ Gemini + Head Agent)"""
    overseas_context = context + "\n(í•´ì™¸ HS ë¶„ë¥˜ ì‚¬ë¡€ ë¶„ì„ ëª¨ë“œ)"
    
    # 5ê°œ ê·¸ë£¹ë³„ë¡œ ê°ê° Geminiì— ë¶€ë¶„ ë‹µë³€ ìš”ì²­
    group_answers = []
    for i in range(5):
        relevant = hs_manager.get_overseas_context_group(user_input, i)
        prompt = f"{overseas_context}\n\nê´€ë ¨ ë°ì´í„° (í•´ì™¸ ê´€ì„¸ì²­, ê·¸ë£¹{i+1}):\n{relevant}\n\nì‚¬ìš©ì: {user_input}\n"
        response = client.models.generate_content(
            model="gemini-2.0-flash",
            contents=prompt
        )
        group_answers.append(clean_text(response.text))

    # Head Agentê°€ 5ê°œ ë¶€ë¶„ ë‹µë³€ì„ ì·¨í•©í•˜ì—¬ ìµœì¢… ë‹µë³€ ìƒì„±
    head_prompt = f"{overseas_context}\n\nì•„ë˜ëŠ” í•´ì™¸ HS ë¶„ë¥˜ ì‚¬ë¡€ ë°ì´í„° 5ê°œ ê·¸ë£¹ë³„ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤. ê° ê·¸ë£¹ì˜ ë‹µë³€ì„ ì¢…í•©í•˜ì—¬ ìµœì¢… ì „ë¬¸ê°€ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.\n\n"
    for idx, ans in enumerate(group_answers):
        head_prompt += f"[ê·¸ë£¹{idx+1} ë‹µë³€]\n{ans}\n\n"
    head_prompt += f"\nì‚¬ìš©ì: {user_input}\n"
    head_response = client.models.generate_content(
        model="gemini-2.5-flash",
        contents=head_prompt
    )
    return clean_text(head_response.text)